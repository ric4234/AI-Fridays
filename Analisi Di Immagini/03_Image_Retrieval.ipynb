{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EuzaOQns_-f"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ric4234/AI-Fridays/blob/main/Analisi%20Di%20Immagini/03_Image_Retrieval.ipynb\" target=\"_parent\\\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Retrieval\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9J0-Rp-6c95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to test how similar the text and image are. The model will output if the text matches the image.\n",
        "\n",
        "In this case we have to use a multimodal model, which is a model that can have inputs of different kind (for example text and image).\n",
        "Some common multimodal task are:\n",
        "\n",
        "*   Image to text matching\n",
        "*   Image captioning\n",
        "*   Visual Q&A\n",
        "*   Zero-Shot image classification\n",
        "\n",
        "For the image to text matching we will use the Blip model from Salesforce (more info at  https://huggingface.co/Salesforce/blip-itm-base-coco)\n",
        "\n"
      ],
      "metadata": {
        "id": "dboJZTu3EW06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Install dependencies and utils functions"
      ],
      "metadata": {
        "id": "OxuauA8fGknc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "ZVOMcYmyrr6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_image_from_url_in_rgb_mode(url):\n",
        "    return Image.open(requests.get(url, stream=True).raw).convert('RGB')"
      ],
      "metadata": {
        "id": "ghmsvvlcHTuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Image to text matching using Blip model"
      ],
      "metadata": {
        "id": "EdVtVpiGGuhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Blip model"
      ],
      "metadata": {
        "id": "rdGIHkBYG_ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipForImageTextRetrieval\n",
        "model = BlipForImageTextRetrieval.from_pretrained(\n",
        "    \"Salesforce/blip-itm-base-coco\")"
      ],
      "metadata": {
        "id": "AqWmj7bgiMnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the processor. The processor will process the text and the image from the model"
      ],
      "metadata": {
        "id": "wCfFxYHgHCLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"Salesforce/blip-itm-base-coco\")"
      ],
      "metadata": {
        "id": "ZBeiDwT2iT-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the image in RGB mode"
      ],
      "metadata": {
        "id": "ei5cecpLLN-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Fetch image from URL\n",
        "url = 'https://www.hallofseries.com/wp-content/uploads/2018/11/boris.jpg'  # Replace with your image URL\n",
        "\n",
        "raw_image = load_image_from_url_in_rgb_mode(url)\n",
        "\n",
        "raw_image"
      ],
      "metadata": {
        "id": "5w6tafqUiicX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the text that will be matched with the previously loaded image"
      ],
      "metadata": {
        "id": "lXWGmgzNLSnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"in this image there is a person and a goldfish\""
      ],
      "metadata": {
        "id": "DRd_-69YioU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model inputs. We need to pass the image, the text and the output that will be returned by the model. In this case is a Pytorch tensor"
      ],
      "metadata": {
        "id": "Wnz6GG9HLakj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=raw_image,\n",
        "                   text=text,\n",
        "                   return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "qkB8onCii8QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs # It is a dictionary of multiple arguments"
      ],
      "metadata": {
        "id": "-OE7rYVMjB1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we pass the inputs to the Blip model previously defined"
      ],
      "metadata": {
        "id": "HPPaZdknL7eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itm_scores = model(**inputs)[0] ## ** is mandatory since we are passing a dictionary that contains the arguments\n",
        "itm_scores"
      ],
      "metadata": {
        "id": "dIdZuR4pjEIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, the numbers does not mean anything. Thats because itm_scores are in the form of logits, which are the raw, unnormalized scores produced by the model. Logits are typically the output of a neural network before applying any activation function, such as softmax. The softmax function is often applied to logits to convert them into probabilities. It normalizes the logits into a probability distribution, ensuring that the sum of the probabilities across all classes is equal to 1. From wikipedia you can find more info on the logit function: https://it.wikipedia.org/wiki/Logit"
      ],
      "metadata": {
        "id": "biEXbkOCM-Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "itm_score = torch.nn.functional.softmax(\n",
        "    itm_scores,dim=1)\n",
        "\n",
        "itm_score"
      ],
      "metadata": {
        "id": "ElyktgtNjKSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\\\n",
        "The image and text are matched \\\n",
        "with a probability of {itm_score[0][1]:.4f}\"\"\")"
      ],
      "metadata": {
        "id": "nJAcq0rCjN5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}