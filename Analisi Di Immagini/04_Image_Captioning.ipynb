{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EuzaOQns_-f"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ric4234/AI-Fridays/blob/main/Analisi%20Di%20Immagini/04_Image_Captioning.ipynb\" target=\"_parent\\\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Captioning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9J0-Rp-6c95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to return the description of an image.\n",
        "\n",
        "In this case we have to use a multimodal model, which is a model that can have inputs of different kind (for example text and image).\n",
        "Some common multimodal task are:\n",
        "\n",
        "*   Image to text matching\n",
        "*   Image captioning\n",
        "*   Visual Q&A\n",
        "*   Zero-Shot image classification\n",
        "\n",
        "For the image capioning we will use the Blip model from Salesforce (more info at https://huggingface.co/Salesforce/blip-image-captioning-base)\n",
        "\n"
      ],
      "metadata": {
        "id": "3zE9KZ6_Pmf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Install dependencies and utils functions"
      ],
      "metadata": {
        "id": "IMOLiqo8P9Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ZVOMcYmyrr6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    return Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "SB_S6NIQQBh3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Image captioning using Blip model"
      ],
      "metadata": {
        "id": "ymXGX3pLQCFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Blip model"
      ],
      "metadata": {
        "id": "xDnMDxOiQeyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipForConditionalGeneration\n",
        "\n",
        "model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "ZMZjRXtikbod"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the processor. The processor will process the text and the image from the model"
      ],
      "metadata": {
        "id": "s7bldq4fQsPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "6KN20z-kkiBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the image in RGB mode"
      ],
      "metadata": {
        "id": "pHa9L76AQ22C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Fetch image from URL\n",
        "url = 'https://www.hallofseries.com/wp-content/uploads/2018/11/boris.jpg'  # Replace with your image URL\n",
        "\n",
        "image = load_image_from_url(url)\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "LRf1vUuQkpXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creat the text that will be used as a conditional image captioning. We also define the model inputs. We need to pass the image, the text and the output that will be returned by the model. In this case is a Pytorch tensor"
      ],
      "metadata": {
        "id": "w9OHYbYBRBzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"a photograph of\"\n",
        "inputs = processor(image, text, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "id": "vDSrgIL5lYk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we pass the inputs to the Blip model previously defined"
      ],
      "metadata": {
        "id": "IgncG2i6RxOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(**inputs) # ** is mandatory since we are passing a dictionary that contains the arguments"
      ],
      "metadata": {
        "id": "RxATUOJdldm-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "id": "0bAWdh2jlgc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output are numbers, in this case they are Token Id, which is how the model understands the text. Each token represent a part of a word or sometimes a single word. To decode those tokens we need to call the decode method from the processor"
      ],
      "metadata": {
        "id": "0tbOZm0oSK15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ZjhBfYLYlijV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to run the model without text conditions: in this case the model retrieves the entire description of the picture"
      ],
      "metadata": {
        "id": "5hlE1tNRS1r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(image,return_tensors=\"pt\")\n",
        "\n",
        "out = model.generate(**inputs)\n",
        "\n",
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "46DT6VBpllm3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}