{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EuzaOQns_-f"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ric4234/AI-Fridays/blob/main/Analisi%20Di%20Immagini/05_Multimodal_Visual_Question_Answering.ipynb\" target=\"_parent\\\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visual Question & Answering\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9J0-Rp-6c95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is create a model where we can ask questions about a picture.\n",
        "\n",
        "In this case we have to use a multimodal model, which is a model that can have inputs of different kind (for example text and image).\n",
        "Some common multimodal task are:\n",
        "\n",
        "*   Image to text matching\n",
        "*   Image captioning\n",
        "*   Visual Q&A\n",
        "*   Zero-Shot image classification\n",
        "\n",
        "For the image Visual Q&A we will use the Blip model from Salesforce (more info at https://huggingface.co/Salesforce/blip-vqa-base)"
      ],
      "metadata": {
        "id": "nDm1Lad4TmPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Install dependencies and utils functions"
      ],
      "metadata": {
        "id": "9JxXMNQlUhfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ZVOMcYmyrr6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    return Image.open(requests.get(url, stream=True).raw).convert('RGB')"
      ],
      "metadata": {
        "id": "rnFDAgUTUlXv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Visual Question & Answering using Blip model"
      ],
      "metadata": {
        "id": "_bJ7BDJ6Uolk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Blip model"
      ],
      "metadata": {
        "id": "dPVGVOygVuYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipForQuestionAnswering\n",
        "model = BlipForQuestionAnswering.from_pretrained(\n",
        "    \"Salesforce/blip-vqa-base\")"
      ],
      "metadata": {
        "id": "pv-8QzoKtOii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the processor. The processor will process the text and the image from the model"
      ],
      "metadata": {
        "id": "OMmYSWM3V1DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"Salesforce/blip-vqa-base\")"
      ],
      "metadata": {
        "id": "El1ekJCdmoI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the image in RGB mode"
      ],
      "metadata": {
        "id": "yjm2APuXWAbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Fetch image from URL\n",
        "url = 'https://www.hallofseries.com/wp-content/uploads/2018/11/boris.jpg'  # Replace with your image URL\n",
        "\n",
        "image = load_image_from_url(url)\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "vbzVAPWDmvZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's prepare a question to ask to the model"
      ],
      "metadata": {
        "id": "Sj0ssQmKWZmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how many persons are in the picture?\""
      ],
      "metadata": {
        "id": "Q7_MGL1om8BJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define the model inputs. We need to pass the image, the text and the output that will be returned by the model. In this case is a Pytorch tensor"
      ],
      "metadata": {
        "id": "iPhCIE5cWl8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(image, question, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "id": "FL3Uq7B5nEVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we pass the inputs to the Blip model previously defined"
      ],
      "metadata": {
        "id": "08ugAoZBWzxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(**inputs) # ** is mandatory since we are passing a dictionary that contains the arguments\n",
        "out"
      ],
      "metadata": {
        "id": "v0rVPbtBnFn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output are numbers, in this case they are Token Id, which is how the model understands the text. Each token represent a part of a word or sometimes a single word. To decode those tokens we need to call the decode method from the processor"
      ],
      "metadata": {
        "id": "SGtwUoe4W-Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "lWQllwUPnG5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}