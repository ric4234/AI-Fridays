{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EuzaOQns_-f"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ric4234/AI-Fridays/blob/main/Analisi%20Di%20Immagini/06_Zero_Shot_Image_Classification.ipynb\" target=\"_parent\\\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero Shot Image Classification\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9J0-Rp-6c95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise to create a model able to classify an image from a list of any labels given to it. More specifically, it will classify the most likely label.\n",
        "\n",
        "In this case we have to use a multimodal model, which is a model that can have inputs of different kind (for example text and image).\n",
        "Some common multimodal task are:\n",
        "\n",
        "*   Image to text matching\n",
        "*   Image captioning\n",
        "*   Visual Q&A\n",
        "*   Zero-Shot image classification\n",
        "\n",
        "For Zero-Shot image classification we will use the CLIP model from OpenAI (more info at https://huggingface.co/openai/clip-vit-large-patch14)"
      ],
      "metadata": {
        "id": "Jurh8KiwXUqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Install dependencies and utils functions"
      ],
      "metadata": {
        "id": "8VWd6xBwXnqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ZVOMcYmyrr6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    return Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "yLmf9MVRXshG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Zero-Shot image classification using CLIP model"
      ],
      "metadata": {
        "id": "bmeZ8auNXwLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load CLIP model"
      ],
      "metadata": {
        "id": "5X7HElUkYG4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\n",
        "    \"openai/clip-vit-large-patch14\")"
      ],
      "metadata": {
        "id": "UeGslzKbptPY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the processor. The processor will process the text and the image from the model"
      ],
      "metadata": {
        "id": "h6izClTnZGvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"openai/clip-vit-large-patch14\")"
      ],
      "metadata": {
        "id": "jqQeN6CjpyLw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the image"
      ],
      "metadata": {
        "id": "spOVivl4ZTQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Fetch image from URL\n",
        "url = 'https://www.hallofseries.com/wp-content/uploads/2018/11/boris.jpg'  # Replace with your image URL\n",
        "\n",
        "image = load_image_from_url(url)\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "mIe-lMOAp8I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the labels object"
      ],
      "metadata": {
        "id": "WnrjrZvgcfnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"a photo of a waterfall\", \"a photo of a goldfish\"]"
      ],
      "metadata": {
        "id": "ARD5JG08qCYh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define the model inputs. We need to pass the image, the text and the output that will be returned by the model. In this case is a Pytorch tensor"
      ],
      "metadata": {
        "id": "JYSZ8j0cckd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(text=labels,\n",
        "                   images=image,\n",
        "                   return_tensors=\"pt\",\n",
        "                   padding=True) # Add this parameters in case the labels are not the same\n",
        "inputs"
      ],
      "metadata": {
        "id": "iPKLfopKcc0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we pass the inputs to the CLIP model previously defined"
      ],
      "metadata": {
        "id": "mXFcw_7CczZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "outputs"
      ],
      "metadata": {
        "id": "hcGKHuuhqGdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are interested in the logits per image. Logits are the raw, unnormalized scores produced by the model. Logits are typically the output of a neural network before applying any activation function, such as softmax. The softmax function is often applied to logits to convert them into probabilities."
      ],
      "metadata": {
        "id": "9EgRCmFCdHdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.logits_per_image"
      ],
      "metadata": {
        "id": "wt7LveNlqLtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert logits to show probability"
      ],
      "metadata": {
        "id": "Lms4Wwm7dftR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs = outputs.logits_per_image.softmax(dim=1)[0]"
      ],
      "metadata": {
        "id": "uZ1M6W7zqODS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "id": "VWQFYO79qP1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = list(probs)\n",
        "for i in range(len(labels)):\n",
        "  print(f\"label: {labels[i]} - probability of {probs[i].item():.4f}\")"
      ],
      "metadata": {
        "id": "FybB_91xqRrK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}