{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EuzaOQns_-f"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ric4234/AI-Fridays/blob/main/Analisi%20Di%20Immagini/01_NLP.ipynb\" target=\"_parent\\\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Detection with Transformers\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9J0-Rp-6c95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to build a model capable of understanding what a figure displays.\n",
        "\n",
        "More specifically, Object Detection consist in detecting all objects in a image. Object Detection combines both classification and localization, because for each object you detect in an image you have also to provide label of the instance.\n",
        "\n",
        "To do so, we will build a pipeline object from Transformers. Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs and also save the time and resources required to train a model from scratch.\n",
        "At this link you can find more about Transformers: https://huggingface.co/docs/transformers/index.\n"
      ],
      "metadata": {
        "id": "X5vwStcT6KPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Install dependencies and create utils functions"
      ],
      "metadata": {
        "id": "bpDLN98uAMzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we make sure to install all the needed libraries"
      ],
      "metadata": {
        "id": "JMFrOUcb62Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install gradio\n",
        "!pip install timm\n",
        "!pip install inflect\n",
        "!pip install phonemizer\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "ZVOMcYmyrr6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code contains few utils function that will be used in the nexts sections, like rendering the object detection result in an image or loading an image from URL"
      ],
      "metadata": {
        "id": "g5eu22dx8R2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import inflect\n",
        "from PIL import Image\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    return Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "def render_results_in_image(in_pil_img, in_results):\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    plt.imshow(in_pil_img)\n",
        "\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for prediction in in_results:\n",
        "\n",
        "        x, y = prediction['box']['xmin'], prediction['box']['ymin']\n",
        "        w = prediction['box']['xmax'] - prediction['box']['xmin']\n",
        "        h = prediction['box']['ymax'] - prediction['box']['ymin']\n",
        "\n",
        "        ax.add_patch(plt.Rectangle((x, y),\n",
        "                                   w,\n",
        "                                   h,\n",
        "                                   fill=False,\n",
        "                                   color=\"green\",\n",
        "                                   linewidth=2))\n",
        "        ax.text(\n",
        "           x,\n",
        "           y,\n",
        "           f\"{prediction['label']}: {round(prediction['score']*100, 1)}%\",\n",
        "           color='red'\n",
        "        )\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Save the modified image to a BytesIO object\n",
        "    img_buf = io.BytesIO()\n",
        "    plt.savefig(img_buf, format='png',\n",
        "                bbox_inches='tight',\n",
        "                pad_inches=0)\n",
        "    img_buf.seek(0)\n",
        "    modified_image = Image.open(img_buf)\n",
        "\n",
        "    # Close the plot to prevent it from being displayed\n",
        "    plt.close()\n",
        "\n",
        "    return modified_image\n",
        "\n",
        "def summarize_predictions_natural_language(predictions):\n",
        "    summary = {}\n",
        "    p = inflect.engine()\n",
        "\n",
        "    for prediction in predictions:\n",
        "        label = prediction['label']\n",
        "        if label in summary:\n",
        "            summary[label] += 1\n",
        "        else:\n",
        "            summary[label] = 1\n",
        "\n",
        "    result_string = \"In this image, there are \"\n",
        "    for i, (label, count) in enumerate(summary.items()):\n",
        "        count_string = p.number_to_words(count)\n",
        "        result_string += f\"{count_string} {label}\"\n",
        "        if count > 1:\n",
        "          result_string += \"s\"\n",
        "\n",
        "        result_string += \" \"\n",
        "\n",
        "        if i == len(summary) - 2:\n",
        "          result_string += \"and \"\n",
        "\n",
        "    # Remove the trailing comma and space\n",
        "    result_string = result_string.rstrip(', ') + \".\"\n",
        "\n",
        "    return result_string\n",
        "\n",
        "\n",
        "##### To ignore warnings #####\n",
        "import warnings\n",
        "import logging\n",
        "from transformers import logging as hf_logging\n",
        "\n",
        "def ignore_warnings():\n",
        "    # Ignore specific Python warnings\n",
        "    warnings.filterwarnings(\"ignore\", message=\"Some weights of the model checkpoint\")\n",
        "    warnings.filterwarnings(\"ignore\", message=\"Could not find image processor class\")\n",
        "    warnings.filterwarnings(\"ignore\", message=\"The `max_size` parameter is deprecated\")\n",
        "\n",
        "    # Adjust logging for libraries using the logging module\n",
        "    logging.basicConfig(level=logging.ERROR)\n",
        "    hf_logging.set_verbosity_error()\n",
        "\n",
        "########"
      ],
      "metadata": {
        "id": "Ge0O1Sl0GJ-7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Build and use Object-Detection Pipeline\n",
        "\n",
        "At this point, we create the object detection pipeline using a model from facebook, which is the detr-resnet-50. For more informations about this model you can see the documentation on Huggingface https://huggingface.co/facebook/detr-resnet-50-dc5.\n",
        "You can find also a lot of other object detection model from Huggingface hub: https://huggingface.co/models?pipeline_tag=object-detection. To choose a model, you can use some metrics like the number of download, likes, or the evaulation metrics provided by the creator of the model"
      ],
      "metadata": {
        "id": "mSgViB5Y8w5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "od_pipe = pipeline(\"object-detection\", \"facebook/detr-resnet-50\") # Add , device = \"cuda:0\" if this is running on a GPU"
      ],
      "metadata": {
        "id": "pv-8QzoKtOii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we load the image and we use the pipeline object created below (od pipe) to detect objects in the choosen image. Objects will be saved in the ouput of the pipeline"
      ],
      "metadata": {
        "id": "a0GDpmas_YsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Fetch image from URL\n",
        "url = 'https://decider.com/wp-content/uploads/2016/09/it-crowd-cult-corner.jpg'  # Replace with your image URL\n",
        "\n",
        "raw_image = load_image_from_url(url)\n",
        "\n",
        "raw_image"
      ],
      "metadata": {
        "id": "NudbNWiYwRBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_output = od_pipe(raw_image)\n",
        "\n",
        "pipeline_output"
      ],
      "metadata": {
        "id": "Jjup1QnHtbUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_image = render_results_in_image(\n",
        "    raw_image,\n",
        "    pipeline_output)\n",
        "processed_image"
      ],
      "metadata": {
        "id": "CS80-QwHzNw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can translate the image in natural language using the summarize_predictions_natural_language method created in the previous section"
      ],
      "metadata": {
        "id": "jZQl4OuhDmi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = summarize_predictions_natural_language(pipeline_output)\n",
        "text"
      ],
      "metadata": {
        "id": "f-8QDCy_Dqg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3 - Create a simple interface using Gradio\n",
        "\n",
        "In this section, we will use Gradio to build a simple user interface (UI) where we can pass an image and then use the pipeline created below to get the result image.\n",
        "\n",
        "Gradio interface expects a method where you pass the input (in this case an image) and return the output (the processed image), so the first thing that we will do is to create that method. You can explore more about Gradio on the official documentation: https://www.gradio.app/docs/interface"
      ],
      "metadata": {
        "id": "TY2-zE1LA1ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "def get_pipeline_prediction(pil_image):\n",
        "\n",
        "    pipeline_output = od_pipe(pil_image)\n",
        "\n",
        "    processed_image = render_results_in_image(pil_image,\n",
        "                                            pipeline_output)\n",
        "    return processed_image"
      ],
      "metadata": {
        "id": "tGUubfpazaO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the method is created, we have to build a Gradio Interface object that will use that method"
      ],
      "metadata": {
        "id": "1CZmVYCvCIhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "  fn=get_pipeline_prediction,\n",
        "  inputs=gr.Image(label=\"Input image\",\n",
        "                  type=\"pil\"), # Python Image Library Object\n",
        "  outputs=gr.Image(label=\"Output image with predicted instances\",\n",
        "                   type=\"pil\") # Python Image Library Object\n",
        ")"
      ],
      "metadata": {
        "id": "BrfBJdtezkfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will launch the Interface. Launch the Interface means to create a simple web server endpoint that you can use to process your own images, directly or via API.\n",
        "\n"
      ],
      "metadata": {
        "id": "xsZt3MgSC-rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "nENY1QRGzs6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to close you demo if you don't want to use it anymore"
      ],
      "metadata": {
        "id": "ja7JWBtCD4Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.close()"
      ],
      "metadata": {
        "id": "2Y54Nrb517i2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
