{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ARIMA Model\n",
        "\n",
        "The ARIMA model is short for the Autoregressive (AR), Integrated (I) Moving Average (MA) model.\n",
        "Specifically, The “I” in ARIMA is for integrating. Integrating is a mathematical synonym\n",
        "for differencing a non-stationary time series. In ARIMA, this differencing is not anymore\n",
        "done in advance of the modeling phase, but it is done during the model fit.\n",
        "\n",
        "The linear trend is a great example of this. More specifically, the values will not be stationary at all: they will\n",
        "augment (or diminish) infinitely. Yet the difference between each value and the next is constant, so the\n",
        "differenced time series is perfectly stationary.\n",
        "\n"
      ],
      "metadata": {
        "id": "9J0-Rp-6c95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "FTeMiwZRZ4wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recupero dati tramite Spark**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "KCAuki_MZ4wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "\n",
        "//val sql_pool_name = \"bi-we-prd1-synws.sql.azuresynapse.net\"\n",
        "val sql_pool_name = mssparkutils.env.getWorkspaceName()+\".sql.azuresynapse.net\"\n",
        "val db_name = \"bi_we_prd1_synws_sqlpool\"\n",
        "val schema_name = \"ml\"\n",
        "val table_name = \"vw_fact_modello_forecast_spedizioni_training_set_numero_spedizioni_daily_all_clienti\"\n",
        "\n",
        "val df = spark.read.sqlanalytics(s\"$db_name.$schema_name.$table_name\")\n",
        "df.createOrReplaceTempView( \"training_table_numero_spedizioni_daily_all_clienti\" )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Cell magic `%%spark` not found.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2ctAuR0Z4wd",
        "outputId": "1b37f9d5-6394-4c2b-a3a1-7d0a1649a243"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creazione funzione di partizionamento**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "JVR04gy8Z4wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comando SQL per selezionare il training set\n",
        "sql = 'select * from training_table_numero_spedizioni_daily_all_clienti order by unique_id, ds'\n",
        "\n",
        "# Parizionamento dei dati per la chiave univoca per creare esecuzioni parallele del modello\n",
        "unique_id_pd = (spark.sql( sql ).repartition(spark.sparkContext.defaultParallelism, ['unique_id'])).cache()\n",
        "\n",
        "unique_id_pd = unique_id_pd.toPandas()\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f5272d11389a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Parizionamento dei dati per la chiave univoca per creare esecuzioni parallele del modello\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0munique_id_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msql\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'unique_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0munique_id_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_id_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "nF5jEFNcZ4wf",
        "outputId": "84440665-6401-4ebd-8f4c-6aca545f823e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Esecuzione del modello (parametri scelti da cross validazione)**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "87gCTAXfZ4wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from prophet import Prophet\n",
        "from prophet.make_holidays import make_holidays_df\n",
        "\n",
        "\n",
        "yearly_seasonality = unique_id_pd['yearly_seasonality'].iloc[0]\n",
        "yearly_seasonality_fourier_order = unique_id_pd['yearly_seasonality_fourier_order'].iloc[0]\n",
        "changepoint_prior_scale = unique_id_pd['changepoint_prior_scale'].iloc[0]\n",
        "seasonality_prior_scale = unique_id_pd['seasonality_prior_scale'].iloc[0]\n",
        "seasonality_mode = unique_id_pd['seasonality_mode'].iloc[0]\n",
        "forecast_horizon = unique_id_pd['forecast_horizon'].iloc[0]\n",
        "weekly_seasonality_fourier_order = unique_id_pd['weekly_seasonality_fourier_order'].iloc[0]\n",
        "\n",
        "model = Prophet(\n",
        "        #interval_width=0.85\n",
        "     # ,\n",
        "        yearly_seasonality= yearly_seasonality_fourier_order\n",
        "      , weekly_seasonality=weekly_seasonality_fourier_order\n",
        "      , changepoint_prior_scale = changepoint_prior_scale\n",
        "      , seasonality_prior_scale = seasonality_prior_scale\n",
        "      ,  seasonality_mode = seasonality_mode)\n",
        "\n",
        "model.add_country_holidays(country_name='IT')\n",
        "\n",
        "#model.add_seasonality(name='year', period=12, fourier_order = 10)\n",
        "model.fit( unique_id_pd )\n",
        "\n",
        "future_pd = model.make_future_dataframe(\n",
        "periods=forecast_horizon,\n",
        "freq='D',\n",
        "include_history=True\n",
        ")\n",
        "\n",
        "forecast_pd = model.predict( future_pd )\n",
        "\n",
        "f_pd = forecast_pd.set_index('ds')\n",
        "\n",
        "\n",
        "fig = model.plot(forecast_pd, xlabel='ds',\n",
        "ylabel=r'yhat')\n",
        "\n",
        "fig2 = model.plot_components(forecast_pd)\n",
        "fig2.show()\n",
        "\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "dtTkKF_TZ4wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creazione vista temporanea sui risultati**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "rlsf-9vuZ4wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Memorizzazione dati**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "jLE3dguEZ4wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comando SQL per selezionare il training set\n",
        "sql = 'select * from training_table_numero_spedizioni_daily_all_clienti order by unique_id, ds'\n",
        "\n",
        "# Parizionamento dei dati per la chiave univoca per creare esecuzioni parallele del modello\n",
        "unique_id_part = (spark.sql( sql ).repartition(spark.sparkContext.defaultParallelism, ['unique_id'])).cache()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "lst8kGDEZ4wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.types import *\n",
        "from prophet import Prophet\n",
        "from prophet.make_holidays import make_holidays_df\n",
        "\n",
        "result_schema =StructType([\n",
        "  StructField('ds',TimestampType()),\n",
        "  StructField('unique_id',IntegerType()),\n",
        "  #StructField('y',DoubleType()),\n",
        "  StructField('yhat',DoubleType()),\n",
        "  StructField('yhat_upper',DoubleType()),\n",
        "  StructField('yhat_lower',DoubleType())\n",
        "  ])\n",
        "\n",
        "\n",
        "# Creazione udf in modo che viene eseguita in parallelo per ogni serie\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "\n",
        "@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )\n",
        "def forecast_sales( unique_id_pd ):\n",
        "\n",
        "  it_holidays = make_holidays_df(\n",
        "    year_list=[2021 + i for i in range(5)], country='IT'\n",
        "  )\n",
        "\n",
        "  yearly_seasonality = unique_id_pd['yearly_seasonality'].iloc[0]\n",
        "  yearly_seasonality_fourier_order = unique_id_pd['yearly_seasonality_fourier_order'].iloc[0]\n",
        "  changepoint_prior_scale = unique_id_pd['changepoint_prior_scale'].iloc[0]\n",
        "  seasonality_prior_scale = unique_id_pd['seasonality_prior_scale'].iloc[0]\n",
        "  seasonality_mode = unique_id_pd['seasonality_mode'].iloc[0]\n",
        "  forecast_horizon = unique_id_pd['forecast_horizon'].iloc[0]\n",
        "  weekly_seasonality_fourier_order = unique_id_pd['weekly_seasonality_fourier_order'].iloc[0]\n",
        "\n",
        "  model = Prophet(\n",
        "        interval_width=0.85\n",
        "      , yearly_seasonality= yearly_seasonality_fourier_order\n",
        "      , weekly_seasonality=weekly_seasonality_fourier_order\n",
        "      , changepoint_prior_scale = changepoint_prior_scale\n",
        "      , seasonality_prior_scale = seasonality_prior_scale\n",
        "      ,  seasonality_mode = seasonality_mode)\n",
        "\n",
        "  #if yearly_seasonality:\n",
        "    #model.add_seasonality(name='yearly', period=7 * 52, fourier_order = 10)\n",
        "\n",
        "  model.add_country_holidays(country_name='IT')\n",
        "\n",
        "\n",
        "  model.fit( unique_id_pd )\n",
        "\n",
        "  future_pd = model.make_future_dataframe(\n",
        "    periods=forecast_horizon,\n",
        "    freq='D',\n",
        "    include_history=False\n",
        "    )\n",
        "\n",
        "  forecast_pd = model.predict( future_pd )\n",
        "\n",
        "  f_pd = forecast_pd[ ['ds','yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\n",
        "\n",
        "  #st_pd = unique_id_pd[['ds','unique_id']].set_index('ds')\n",
        "\n",
        "  #results_pd = f_pd.join( st_pd, how='left' )\n",
        "  results_pd = f_pd\n",
        "  results_pd.reset_index(level=0, inplace=True)\n",
        "\n",
        "  results_pd['unique_id'] = unique_id_pd['unique_id'].iloc[0]\n",
        "\n",
        "  return results_pd[ ['ds', 'unique_id', 'yhat', 'yhat_upper', 'yhat_lower'] ]\n",
        "\n",
        "\n",
        "# forecasting\n",
        "results = (\n",
        "unique_id_part\n",
        ".groupBy('unique_id')\n",
        ".apply(forecast_sales))\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "xdun1GdkZ4wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results.cache()\n",
        "\n",
        "results.createOrReplaceTempView(\"modello_forecast_spedizioni_numero_spedizioni_daily_all_clienti\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "4j0x5s1BZ4wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Popolamento tabella di output**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "D6DGiDdOZ4wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%spark\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "\n",
        "//val sql_pool_name = \"bi-we-prd1-synws.sql.azuresynapse.net\"\n",
        "val sql_pool_name = mssparkutils.env.getWorkspaceName()+\".sql.azuresynapse.net\"\n",
        "val db_name = \"bi_we_prd1_synws_sqlpool\"\n",
        "val schema_name = \"ml\"\n",
        "val table_name = \"fact_modello_forecast_spedizioni_numero_spedizioni_daily_all_clienti_stg\"\n",
        "\n",
        "// Scrittura nel SQL pool\n",
        "spark.sqlContext.sql(\"SELECT ds, cast(unique_id as int) as unique_id, cast(yhat as float) as y, cast(yhat_upper as float) as y_upper, cast(yhat_lower as float) as y_lower FROM modello_forecast_spedizioni_numero_spedizioni_daily_all_clienti\").write.mode(\"overwrite\").option(Constants.SERVER,s\"$sql_pool_name\").synapsesql(s\"$db_name.$schema_name.$table_name\" , Constants.INTERNAL)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "id": "6WWpSK3aZ4wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Y2xMrJHkZ4wj"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}